# ğŸ“Š dbt Data Pipeline

Welcome to the dbt transformation layer of this project! This directory contains all SQL models, tests, and configurations for transforming raw Snowflake data into analytics-ready tables.

## ğŸ¯ Project Overview

This dbt project transforms raw TPCH data into structured staging and mart layers, orchestrated by Airflow using Astronomer Cosmos.

## ğŸ“ Project Structure

```
data_pipeline/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/              # Staging models (raw â†’ cleaned)
â”‚   â”‚   â”œâ”€â”€ stg_tpch_orders.sql
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ marts/                # Business logic models
â”‚       â””â”€â”€ ...
â”œâ”€â”€ tests/                    # Data quality tests
â”œâ”€â”€ macros/                   # Reusable SQL functions
â”œâ”€â”€ dbt_project.yml          # Project configuration
â”œâ”€â”€ packages.yml             # dbt dependencies
â””â”€â”€ README.md                # This file
```

## ğŸš€ Quick Start

### Running Locally (Optional)

If you want to test dbt models locally before deploying to Airflow:

```bash
# Navigate to dbt project directory
cd dags/dbt/data_pipeline

# Install dependencies
dbt deps

# Run all models
dbt run

# Run specific model
dbt run --select stg_tpch_orders

# Run tests
dbt test

# Generate documentation
dbt docs generate
dbt docs serve
```

### Running via Airflow (Recommended)

In production, all dbt commands are orchestrated by Airflow:

1. Trigger the `dbt_dag` in Airflow UI
2. Monitor execution in the task logs
3. Check Snowflake for transformed tables

## ğŸ› ï¸ Development Workflow

### 1. Create a New Model

```bash
# Create a new SQL file in models/
touch models/staging/stg_new_model.sql
```

Example model:
```sql
{{ config(materialized='view') }}

SELECT
    id,
    name,
    created_at
FROM {{ source('raw', 'table_name') }}
WHERE deleted_at IS NULL
```

### 2. Test Locally

```bash
dbt run --select stg_new_model
dbt test --select stg_new_model
```

### 3. Deploy via Airflow

Commit your changes and Airflow will automatically pick up the new model in the next DAG run.

## ğŸ“¦ Dependencies

Defined in `packages.yml`:

```yaml
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1
```

Install with: `dbt deps`

## âœ… Testing

### Schema Tests

Defined in `schema.yml`:

```yaml
models:
  - name: stg_tpch_orders
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
```

### Data Tests

Custom SQL tests in `tests/` folder that return failing rows.

## ğŸ¨ Model Materialization

Configure in each model:

- **`view`**: Fast, no storage, rebuilds on query
- **`table`**: Slower build, faster queries
- **`incremental`**: Only processes new/changed data
- **`ephemeral`**: CTEs, no database object

Example:
```sql
{{ config(materialized='table') }}
```

## ğŸ“Š Model Lineage

View the full lineage graph:
```bash
dbt docs generate
dbt docs serve
```

Or check the Airflow Graph view to see task dependencies.

## ğŸ”§ Configuration

### profiles.yml (Auto-generated by Cosmos)

When running via Airflow, Cosmos automatically generates the profile configuration using the `snowflake_conn` connection. No manual configuration needed!

### dbt_project.yml

Main project settings:
```yaml
name: 'data_pipeline'
version: '1.0.0'
profile: 'default'

models:
  data_pipeline:
    staging:
      +materialized: view
    marts:
      +materialized: table
```

## ğŸ“ˆ Best Practices

1. **Staging Models**: Clean and standardize raw data
   - Rename columns to consistent format
   - Cast data types
   - Filter out deleted records
   - No business logic

2. **Mart Models**: Implement business logic
   - Joins and aggregations
   - Calculate metrics
   - Create dimension and fact tables

3. **Testing**: Add tests for critical columns
   - Primary keys: `unique` and `not_null`
   - Referential integrity: `relationships`
   - Business rules: custom tests

4. **Documentation**: Document models in `schema.yml`
   ```yaml
   models:
     - name: stg_tpch_orders
       description: "Staging table for TPCH orders"
   ```

## ğŸ› Troubleshooting

### Model Fails in Airflow

1. Check task logs in Airflow UI
2. Look for SQL errors or data issues
3. Test locally: `dbt run --select failing_model`

### Dependencies Not Installing

```bash
# Clear dbt cache
rm -rf dbt_packages/
rm -rf target/

# Reinstall
dbt deps
```

### Profile Connection Issues

Verify Snowflake connection in Airflow:
```bash
astro dev bash
airflow connections test snowflake_conn
```

## ğŸ“š Resources

- [dbt Documentation](https://docs.getdbt.com)
- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices)
- [dbt Discourse Community](https://discourse.getdbt.com)
- [dbt Slack Community](https://community.getdbt.com)

## ğŸ¤ Contributing

When adding new models:
1. Follow the naming convention: `stg_`, `int_`, `fct_`, `dim_`
2. Add tests and documentation
3. Test locally before committing
4. Verify in Airflow after deployment

